{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a2c366-f3ef-42d9-a6cf-58e5ac000fdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from mne.preprocessing import ICA\n",
    "\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import kurtosis, skew\n",
    "import pywt\n",
    "import antropy as ant"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "02d0fc52-aacb-4c5b-bc62-49e763a9331e",
   "metadata": {},
   "source": [
    "Firstly, a single EEG is imported to explore its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2148c902-f5e7-41f4-9690-706e56c64b8d",
   "metadata": {},
   "source": [
    "raw = mne.io.read_raw_edf('data/MODMA_EEG_BIDS_format/EEG_LZU_2015_2_resting state/sub-001/eeg/sub-001_task-Resting-state_eeg.EDF')\n",
    "raw.describe()\n",
    "print(raw.ch_names)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "198e248a-fadd-47c8-b478-4c4b0fba3295",
   "metadata": {},
   "source": [
    "Convert the electrode positions Excel file to a CSV one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd823b9-1977-42dd-8c65-33a6d07fccc5",
   "metadata": {},
   "source": [
    "excel_data = pd.read_excel(\"data/MODMA_EEG_BIDS_format/EEG_LZU_2015_2_resting state/subjects_information_EEG_128channels_resting_lanzhou_2015.xlsx\")\n",
    "excel_data.to_csv(\"data/MODMA_EEG_BIDS_format/EEG_LZU_2015_2_resting state/subject_information.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a041f8ce-d638-4620-99ad-edbf4dc43e27",
   "metadata": {},
   "source": [
    "Here the feature extraction functions for the raw EEG data are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d814b9-3132-4517-b05f-63c3ddf06277",
   "metadata": {
    "tags": []
   },
   "source": [
    "def band_powers_ratios(raw_frame, explicit=True): # raw frame is 128 x 75250\n",
    "        \n",
    "    freqs, psd = welch(raw_frame, fs=250, axis=1)\n",
    "\n",
    "    freq_bands = {\n",
    "        \"Delta\": [1, 4], \n",
    "        \"Theta\": [4, 8],\n",
    "        \"Alpha\": [8, 13],\n",
    "        \"Beta\":  [13, 30],\n",
    "        \"Gamma\": [30, 45]\n",
    "        }\n",
    "\n",
    "    # Calculate global average band power for entire sample\n",
    "    powers = {}\n",
    "    for band, (l, h) in freq_bands.items():\n",
    "        mask = (freqs >= l) & (freqs <= h)\n",
    "        # Take average across both frequencies and channel dimensions to generate a single global average band power value\n",
    "        powers[band] = 10 * np.log10(np.mean(psd[:, mask]) + 1e-12)  # Avoid log(0) and convert to decibel (standard for PSD)\n",
    "    \n",
    "    # Calculate ratios between global average band powers\n",
    "    ratios = {\n",
    "        f\"{b1}/{b2}\": powers[b1] / powers[b2]\n",
    "        for i, b1 in enumerate(powers)\n",
    "        for j, b2 in enumerate(powers)\n",
    "        if i != j\n",
    "    }\n",
    "    \n",
    "    merged = powers | ratios\n",
    "    merged_dataframe = pd.DataFrame([merged])\n",
    "    \n",
    "    if explicit:\n",
    "        print(merged_dataframe.shape)\n",
    "        print(merged_dataframe)\n",
    "        \n",
    "    return merged_dataframe\n",
    "\n",
    "\n",
    "# Discrete Wavelet Transform\n",
    "def db_wavelet_features(raw_frame, wavelet=\"db4\", level=4, explicit=True):\n",
    "    features = []\n",
    "    # Perform Discrete Wavelet Transform using Daubechies\n",
    "    coefs = pywt.wavedec(raw_frame, wavelet=wavelet, level=level)\n",
    "    for i, c in reversed(list(enumerate(coefs))):\n",
    "        features.extend([\n",
    "            np.sqrt(np.mean(np.square(c))),\n",
    "            kurtosis(c.flatten()),\n",
    "            skew(c.flatten())\n",
    "        ])\n",
    "        \n",
    "    # Extracting global wavelet entropy\n",
    "    \n",
    "    # Calcuate energy per wavelet\n",
    "    energies = np.array([np.sum(np.square(c)) for c in coefs])\n",
    "    # See how much they contribute to the overall signal\n",
    "    distribution = energies / (np.sum(energies)+1e-12)\n",
    "    # Calculate Shannon entropy on the distribution\n",
    "    entropy = -np.sum(distribution * (np.log2(distribution)+1e-12))\n",
    "    features.append(entropy) \n",
    "    \n",
    "    # Generate feature labels\n",
    "    labels = []\n",
    "    labels.extend([\"cA4_RMS\", \"cA4_kurtosis\", \"cA4_skew\"])\n",
    "    labels.extend([f\"cD{i}_{metric}\" for i in range(4,0,-1) for metric in [\"RMS\", \"kurtosis\", \"skew\"]])\n",
    "    labels.append(\"wavelet_entropy\")\n",
    "    \n",
    "    # Combine the data with the labels to create a dataframe\n",
    "    final = pd.DataFrame(data=[features], columns=labels)\n",
    "\n",
    "    if explicit:\n",
    "        print(final.shape)\n",
    "        print(final)\n",
    "    \n",
    "    return final\n",
    "\n",
    "def other_metrics(raw_frame, explicit=True):  \n",
    "    head_sections = {\n",
    "        \"frontal\": [31, 24, 20, 13, 7, 0, 25, 21, 14, 8, 1, 26, 22, 17, 15, 9, 2, 23, 18, 10, 3, 123, 122, 124, 127, 16, 126, 125, 11, 4],\n",
    "        \"central\": [19, 5, 117, 28, 12, 111, 110, 35, 29, 6, 105, 104, 103, 41, 36, 33, 79, 86, 92, 52, 53, 54, 78, 85],\n",
    "        \"temporal\": [47, 42, 37, 32, 33, 27, 43, 38, 39, 34, 48, 44, 40, 45, 46, 55, 56, 49, 50, 51, 57, 62, 116, 115, 121, 120, 119, 118, 109, 114, 113, 108, 102, 107, 112, 97, 101, 91, 96, 100, 96, 99, 106, 98],\n",
    "        \"parietal\": [63,58,59,60,64,65,66,69,70,61,71,74,75,82,76,83,77,84,89,90,94],\n",
    "        \"occipital\": [67, 68, 72, 73, 80, 81, 87, 88, 93]\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    passes = []\n",
    "    \n",
    "    # Global metrics (across all electrodes)\n",
    "    # Spectral entropy - calculate for each electrode, then average\n",
    "    spectral_entropies = []\n",
    "    for i in range(raw_frame.shape[0]):\n",
    "        try:\n",
    "            spectral_entropies.append(ant.spectral_entropy(raw_frame.iloc[i, :], sf=250))\n",
    "        except:\n",
    "            passes.append(f\"Spectral entropy pass at i={i}\")\n",
    "    features.append(np.mean(spectral_entropies) if spectral_entropies else np.nan)\n",
    "    \n",
    "    # Permutation entropy - calculate for each electrode, then average\n",
    "    perm_entropies = []\n",
    "    for i in range(raw_frame.shape[0]):\n",
    "        try:\n",
    "            perm_entropies.append(ant.perm_entropy(raw_frame.iloc[i, :]))\n",
    "        except:\n",
    "            passes.append(f\"Permutation entropy pass at i={i}\")\n",
    "    features.append(np.mean(perm_entropies) if perm_entropies else np.nan)\n",
    "    \n",
    "    # Hjorth mobility and complexity - calculate for each electrode, then average\n",
    "    mobility_values = []\n",
    "    complexity_values = []\n",
    "    for i in range(raw_frame.shape[0]):\n",
    "        try:\n",
    "            m, c = ant.hjorth_params(raw_frame.iloc[i, :])\n",
    "            mobility_values.append(m)\n",
    "            complexity_values.append(c)\n",
    "        except:\n",
    "            passes.append(f\"Hjorth pass at i={i}\")\n",
    "    features.append(np.mean(mobility_values) if mobility_values else np.nan)\n",
    "    features.append(np.mean(complexity_values) if complexity_values else np.nan)\n",
    "    \n",
    "    # Zero crossings - calculate for each electrode, then average\n",
    "    zero_crossings = []\n",
    "    for i in range(raw_frame.shape[0]):\n",
    "        try:\n",
    "            zero_crossings.append(ant.num_zerocross(raw_frame.iloc[i, :]))\n",
    "        except:\n",
    "            passes.append(f\"Zero crossings pass at i={i}\")\n",
    "    features.append(np.mean(zero_crossings) if zero_crossings else np.nan)\n",
    "    \n",
    "    # Process each head section\n",
    "    for section, indices in head_sections.items():\n",
    "        # Process metrics for electrodes in this section\n",
    "        sample_entropies = []\n",
    "        higuchi_values = []\n",
    "        dfa_values = []\n",
    "        spectral_entropies_section = []\n",
    "        perm_entropies_section = []\n",
    "        mobility_section = []\n",
    "        complexity_section = []\n",
    "        zero_crossings_section = []\n",
    "        \n",
    "        for idx in indices:\n",
    "            if idx < raw_frame.shape[0]:  # Make sure index is in range\n",
    "                # Get the time series for this electrode\n",
    "                electrode_data = raw_frame.iloc[idx, :].to_numpy(dtype=np.float64)\n",
    "                electrode_data = np.ascontiguousarray(electrode_data)\n",
    "                \n",
    "                # Sample entropy\n",
    "                sample_entropies.append(ant.sample_entropy(electrode_data))\n",
    "\n",
    "                # Higuchi Fractal Dimension\n",
    "                higuchi_values.append(ant.higuchi_fd(electrode_data))\n",
    "\n",
    "                # Detrended fluctuation analysis\n",
    "                dfa_values.append(ant.detrended_fluctuation(electrode_data))\n",
    "\n",
    "                # Spectral entropy for this section\n",
    "                spectral_entropies_section.append(ant.spectral_entropy(electrode_data, sf=250))\n",
    "\n",
    "                # Permutation entropy for this section\n",
    "                perm_entropies_section.append(ant.perm_entropy(electrode_data))\n",
    "\n",
    "                # Hjorth parameters for this section\n",
    "                m, c = ant.hjorth_params(electrode_data)\n",
    "                mobility_section.append(m)\n",
    "                complexity_section.append(c)\n",
    "\n",
    "                # Zero crossings for this section\n",
    "                zero_crossings_section.append(ant.num_zerocross(electrode_data))\n",
    "\n",
    "        # Add average metrics for this section\n",
    "        features.append(np.mean(sample_entropies))\n",
    "        features.append(np.mean(higuchi_values))\n",
    "        features.append(np.mean(dfa_values))\n",
    "        features.append(np.mean(spectral_entropies_section))\n",
    "        features.append(np.mean(perm_entropies_section))\n",
    "        features.append(np.mean(mobility_section))\n",
    "        features.append(np.mean(complexity_section))\n",
    "        features.append(np.mean(zero_crossings_section))\n",
    "    \n",
    "    # Create feature labels\n",
    "    labels.extend([\n",
    "        \"global_spectral_entropy\", \n",
    "        \"global_permutation_entropy\",\n",
    "        \"global_hjorth_mobility\", \n",
    "        \"global_hjorth_complexity\", \n",
    "        \"global_zero_crossings\"\n",
    "    ])\n",
    "    \n",
    "    for section in head_sections.keys():\n",
    "        labels.extend([\n",
    "            f\"{section}_sample_entropy\",\n",
    "            f\"{section}_higuchi\",\n",
    "            f\"{section}_DFA\",\n",
    "            f\"{section}_spectral_entropy\",\n",
    "            f\"{section}_permutation_entropy\",\n",
    "            f\"{section}_hjorth_mobility\",\n",
    "            f\"{section}_hjorth_complexity\",\n",
    "            f\"{section}_zero_crossings\"\n",
    "        ])\n",
    "    \n",
    "    # Replace any NaN values with 0\n",
    "    features = [0 if np.isnan(x) else x for x in features]\n",
    "    \n",
    "    final = pd.DataFrame([features], columns=labels)\n",
    "    # Also save the list of passes\n",
    "    with open(\"passes.txt\", \"w\") as file:\n",
    "        for item in passes:\n",
    "            file.write(f\"{item}\\n\")\n",
    "    \n",
    "    if explicit:\n",
    "        print(final.shape)\n",
    "        print(final)\n",
    "    \n",
    "    return final\n",
    "    \n",
    "def overall_features(raw_frame, explicit=False): # This function combines the prior three feature extaction functions and returns a complete feature set for each data sample\n",
    "    df1 = band_powers_ratios(raw_frame, explicit=explicit)\n",
    "    df2 = db_wavelet_features(raw_frame, explicit=explicit)\n",
    "    df3 = other_metrics(raw_frame, explicit=explicit)\n",
    "\n",
    "    merged = pd.concat([df1, df2, df3], axis=1)\n",
    "    \n",
    "    if explicit:\n",
    "        print(merged.shape)\n",
    "        print()\n",
    "        print(list(merged.columns))\n",
    "        \n",
    "    return merged"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dd914e0e-e0b9-48d3-9670-3665f7bc2172",
   "metadata": {},
   "source": [
    "Make a montage (mapping of electrode positions to scalp) to use later, so that ICA is more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c37ccec-fefe-43f1-b4b7-d9c172e6d595",
   "metadata": {
    "tags": []
   },
   "source": [
    "channel_names = [f\"E{i}\" for i in range(1,129)]\n",
    "\n",
    "# The electrode locations are the same for all subjects\n",
    "pos_file = pd.read_csv(\"data/MODMA_EEG_BIDS_format/EEG_LZU_2015_2_resting state/sub-001/eeg/sub-001_task-Resting-state_electrodes.tsv\", sep=\"\\t\")\n",
    "names = pos_file[\"name\"].str.strip(\"'\").to_list()\n",
    "ch_pos = pos_file[[\"x\",\"y\",\"z\"]].to_numpy()\n",
    "\n",
    "pos_dic = {name:pos for name, pos in zip(names, ch_pos)}\n",
    "montage = mne.channels.make_dig_montage(ch_pos=pos_dic, coord_frame=\"head\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d4520d5a-ac56-4134-8168-9dc8ef9d6786",
   "metadata": {},
   "source": [
    "This is where everything comes together for the preprocessing. All subject directories are iterated through and each EEG file is band-pass and notch filtered, the montage is applied, a synthetic EOG channel is created to remove ocular artifact removal from the data, ICA is ran on that, and the indepndent components are then used to reconstruct the original space. In the end, eveything is re-referenced to the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec861fca-694d-44e3-b5ad-08dc4b220283",
   "metadata": {
    "tags": []
   },
   "source": [
    "def preprocessing():\n",
    "    # Create a list of the paths to each subject folder\n",
    "    sub_folders = []\n",
    "    for filename in os.listdir(\"data/MODMA_EEG_BIDS_format/EEG_LZU_2015_2_resting state/\"):\n",
    "        if \"sub-\" in filename:\n",
    "            sub_folders.append(os.path.join(os.getcwd(), \"data/MODMA_EEG_BIDS_format/EEG_LZU_2015_2_resting state/\", filename))\n",
    "\n",
    "    problematic = [] # potentially missing files\n",
    "    all_data = [] \n",
    "    \n",
    "    # For each subject folder \n",
    "    for subject in (sorted(sub_folders))[:2]:\n",
    "        subject_id = os.path.basename(subject)\n",
    "        edf_path = os.path.join(subject, \"eeg\", f\"{subject_id}_task-Resting-state_eeg.EDF\")\n",
    "        print(edf_path)\n",
    "        # If there is a missing file, skip this session\n",
    "        if not os.path.exists(edf_path):\n",
    "            problematic.append(subject_id)\n",
    "            print(\"PROBLEMATIC: \", problematic[-1])\n",
    "            continue           \n",
    "\n",
    "        raw = mne.io.read_raw_edf(edf_path, preload=True)\n",
    "        raw.filter(1.0, 45.0, picks=\"eeg\") # band-pass filter\n",
    "        raw.notch_filter(50, picks=\"eeg\")  # notch filter power line noise (50hz because data was acquired in China)\n",
    "\n",
    "        raw.set_montage(montage) # set the custom montage\n",
    "        raw = mne.set_bipolar_reference(raw, anode='E22', cathode='E9', ch_name='EOG', drop_refs=False)  # synthetic EOG channel in order to carry out automatic component removal in ICA\n",
    "        ica = ICA(n_components=None, method='fastica', random_state=23, max_iter='auto') # Create ICA object\n",
    "        ica.fit(raw) # Fit it to raw data\n",
    "        eog_inds, _ = ica.find_bads_eog(raw, ch_name='EOG')  # Find bad components based on the synthetic EOG channel we created\n",
    "        ica.exclude.extend(eog_inds) # Remove bad components\n",
    "        raw_clean = ica.apply(raw.copy()) # Reconstruct original space\n",
    "        raw_clean.drop_channels([\"EOG\"]) # remove the synthetic channel we added\n",
    "        raw_clean.set_eeg_reference('average', projection=False) # Re-reference to average\n",
    "\n",
    "        raw_frame = raw_clean.get_data(picks=\"eeg\").T  # Shape: (samples, channels)\n",
    "        pd_frame = pd.DataFrame(raw_frame)\n",
    "        \n",
    "        all_data.append(pd_frame)\n",
    "                \n",
    "    return all_data\n",
    "\n",
    "# mne.set_log_level('WARNING')  # Or 'ERROR' to suppress even more output "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91e74457-e148-454a-814f-efc35be10b93",
   "metadata": {},
   "source": [
    "Compute features from preprocessed data and combine all new features with the ones available for each subject (gender, age, research group, psych test scores, etc...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38d3920f-6598-4f84-96cc-9fbb5f68af31",
   "metadata": {
    "tags": []
   },
   "source": [
    "complete_set = pd.concat([overall_features(pd.DataFrame(x), explicit=False) for x in preprocessing()], ignore_index=True)\n",
    "complete_set.to_csv(\"features_processed_only.csv\", header=True, index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eee8ee43-b9b9-4ac0-8581-855f42bacaba",
   "metadata": {
    "tags": []
   },
   "source": [
    "processed = pd.read_csv(\"features_processed_only.csv\")\n",
    "csv_sub_info = pd.read_csv(\"data/MODMA_EEG_BIDS_format/EEG_LZU_2015_2_resting state/subject_information.csv\")\n",
    "final = pd.concat([csv_sub_info, processed], axis=1)\n",
    "final.to_csv(\"data/MODMA_EEG_BIDS_format/EEG_LZU_2015_2_resting state/full_features.csv\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
