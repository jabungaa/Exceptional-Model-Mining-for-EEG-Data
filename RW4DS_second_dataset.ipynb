{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "07a2c366-f3ef-42d9-a6cf-58e5ac000fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from mne.preprocessing import ICA\n",
    "import time\n",
    "\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import kurtosis, skew\n",
    "import pywt\n",
    "import antropy as ant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d0fc52-aacb-4c5b-bc62-49e763a9331e",
   "metadata": {},
   "source": [
    "Firstly, a single EEG is imported to explore its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36760859-cf3f-4dcd-b240-8cc04a08a521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trying out the Alzheimer's disease, Frontotemporal dementia and Healthy subjects dataset\n",
    "import mne\n",
    "\n",
    "data = mne.io.read_raw_eeglab(\"Alzheimer-Frontotemporal-healthy/derivatives/sub-001/eeg/sub-001_task-eyesclosed_eeg.set\")\n",
    "print(raw._orig_units)\n",
    "# data.describe()\n",
    "# data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a6fea-9e55-485c-b7d2-62eaad74012b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trying out the Alzheimer's disease, Frontotemporal dementia and Healthy subjects dataset\n",
    "import mne\n",
    "import os\n",
    "\n",
    "raw = mne.io.read_raw_eeglab(\"Alzheimer-Frontotemporal-healthy/derivatives/sub-002/eeg/sub-002_task-eyesclosed_eeg.set\")\n",
    "raw.describe()\n",
    "raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041f8ce-d638-4620-99ad-edbf4dc43e27",
   "metadata": {},
   "source": [
    "Here the feature extraction functions for the raw EEG data are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "39d814b9-3132-4517-b05f-63c3ddf06277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def band_powers_ratios(raw_frame, explicit=True): # raw frame is 128 x 75250\n",
    "    # rescale from volts to microvolts, so that final measures are more easily interpretable\n",
    "    freqs, psd = welch(raw_frame, fs=250, nperseg=min(1000, raw_frame.shape[1]), axis=1)\n",
    "\n",
    "    freq_bands = {\n",
    "        \"Delta\": [1, 4], \n",
    "        \"Theta\": [4, 8],\n",
    "        \"Alpha\": [8, 13],\n",
    "        \"Beta\":  [13, 30],\n",
    "        \"Gamma\": [30, 45]\n",
    "        }\n",
    "\n",
    "    # Calculate global average band power for entire sample\n",
    "    powers = {}\n",
    "    for band, (l, h) in freq_bands.items():\n",
    "        mask = (freqs >= l) & (freqs <= h)\n",
    "        # Take average across frequency bands, and then aggregate across channels to obtain a global metric\n",
    "        band_psd = np.mean(psd[:, mask], axis=1) # Shape (n_channels,)\n",
    "        powers[band] = 10 * np.log10(np.mean(band_psd) + 1e-12)  # Avoid log(0) and convert to decibel (standard for PSD)\n",
    "    \n",
    "    # Calculate ratios between global average band powers\n",
    "    powers_linear = {band: 10 ** (val / 10) for band, val in powers.items()}\n",
    "    ratios = {\n",
    "        \"Theta/Alpha\": powers_linear[\"Theta\"] / powers_linear[\"Alpha\"],\n",
    "        \"Theta/Beta\":  powers_linear[\"Theta\"] / powers_linear[\"Beta\"],\n",
    "        \"Delta/Alpha\": powers_linear[\"Delta\"] / powers_linear[\"Alpha\"]\n",
    "    }\n",
    "    \n",
    "    merged = {**powers, **ratios}\n",
    "    merged_dataframe = pd.DataFrame([merged])\n",
    "    \n",
    "    if explicit:\n",
    "        print(merged_dataframe.shape)\n",
    "        print(merged_dataframe)\n",
    "        \n",
    "    return merged_dataframe\n",
    "\n",
    "\n",
    "# Discrete Wavelet Transform\n",
    "def db_wavelet_features(raw_frame, wavelet=\"db4\", level=4, explicit=True):\n",
    "    # Store features for each channel, as it works on 1D data\n",
    "    all_channel_features = []\n",
    "    \n",
    "    for ch_idx in range(raw_frame.shape[0]):\n",
    "        channel_features = []\n",
    "        channel_data = raw_frame.iloc[ch_idx, :].values.astype(np.float32)\n",
    "        \n",
    "        # Perform Discrete Wavelet Transform using Daubechies\n",
    "        coefs = pywt.wavedec(channel_data, wavelet=wavelet, level=level)\n",
    "        for i, c in reversed(list(enumerate(coefs))):\n",
    "            channel_features.extend([np.sqrt(np.mean(np.square(c)))])\n",
    "\n",
    "        # Extracting global wavelet entropy\n",
    "        # Calcuate energy per wavelet\n",
    "        energies = np.array([np.sum(np.square(c)) for c in coefs])\n",
    "        # See how much they contribute to the overall signal\n",
    "        distribution = energies / (np.sum(energies)+1e-12)\n",
    "        # Calculate Shannon entropy on the distribution\n",
    "        entropy = -np.sum(distribution * (np.log2(distribution)+1e-12))\n",
    "        channel_features.append(entropy) \n",
    "        \n",
    "        all_channel_features.append(channel_features)\n",
    "\n",
    "    # Average features across all channels\n",
    "    avg_features = np.mean(all_channel_features, axis=0)\n",
    "    \n",
    "    # Generate feature labels\n",
    "    labels = []\n",
    "    labels.extend([\"cA4_RMS\"])\n",
    "    labels.extend([f\"cD{i}_{'RMS'}\" for i in range(4,0,-1)])\n",
    "    labels.append(\"wavelet_entropy\")\n",
    "\n",
    "    # Combine the data with the labels to create a dataframe\n",
    "    final = pd.DataFrame(data=[avg_features], columns=labels)\n",
    "\n",
    "    if explicit:\n",
    "        print(final.shape)\n",
    "        print(final)\n",
    "    \n",
    "    return final\n",
    "\n",
    "def other_metrics(raw_frame, explicit=True):  \n",
    "    head_sections = {\n",
    "        'frontal': [0, 1, 2, 3, 10, 11, 16],      # Fp1, Fp2, F3, F4, F7, F8, Fz\n",
    "        'central': [4, 5, 17],                     # C3, C4, Cz\n",
    "        'temporal': [12, 13, 14, 15],              # T3, T4, T5, T6\n",
    "        'parietal': [6, 7, 18],                    # P3, P4, Pz\n",
    "        'occipital': [8, 9]                        # O1, O2\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Global metrics (across all electrodes) - averaged across all channels\n",
    "    spectral_entropies = []\n",
    "    perm_entropies = []\n",
    "    mobility_values = []\n",
    "    complexity_values = []\n",
    "    \n",
    "    for ch_idx in range(raw_frame.shape[0]):\n",
    "        channel_data = raw_frame.iloc[ch_idx,:].values.astype(np.float32)\n",
    "        \n",
    "        try:\n",
    "            spectral_entropies.append(ant.spectral_entropy(channel_data, sf=250))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            perm_entropies.append(ant.perm_entropy(channel_data))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            m, c = ant.hjorth_params(channel_data)\n",
    "            mobility_values.append(m)\n",
    "            complexity_values.append(c)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Add global averages\n",
    "    features.extend([\n",
    "        np.mean(spectral_entropies) if spectral_entropies else np.nan,\n",
    "        np.mean(perm_entropies) if perm_entropies else np.nan,\n",
    "        np.mean(mobility_values) if mobility_values else np.nan,\n",
    "        np.mean(complexity_values) if complexity_values else np.nan\n",
    "    ])\n",
    "    \n",
    "    labels.extend([\n",
    "        \"global_spectral_entropy\",\n",
    "        \"global_permutation_entropy\",\n",
    "        \"global_hjorth_mobility\",\n",
    "        \"global_hjorth_complexity\"\n",
    "    ])\n",
    "    \n",
    "    # Process each head section\n",
    "    for section, indices in head_sections.items():\n",
    "        section_metrics = {\n",
    "            'sample_entropy': [],\n",
    "            'higuchi': [],\n",
    "            'dfa': [],\n",
    "            'spectral_entropy': [],\n",
    "            'permutation_entropy': [],\n",
    "            'mobility': [],\n",
    "            'complexity': []\n",
    "        }\n",
    "        \n",
    "        for idx in indices:\n",
    "            electrode_data = raw_frame.iloc[idx, :].values.astype(np.float32)\n",
    "            electrode_data = np.ascontiguousarray(electrode_data)\n",
    "            \n",
    "            try:\n",
    "                section_metrics['sample_entropy'].append(ant.sample_entropy(electrode_data))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                section_metrics['higuchi'].append(ant.higuchi_fd(electrode_data))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                section_metrics['dfa'].append(ant.detrended_fluctuation(electrode_data))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                section_metrics['spectral_entropy'].append(\n",
    "                    ant.spectral_entropy(electrode_data, sf=250))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                section_metrics['permutation_entropy'].append(\n",
    "                    ant.perm_entropy(electrode_data))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                m, c = ant.hjorth_params(electrode_data)\n",
    "                section_metrics['mobility'].append(m)\n",
    "                section_metrics['complexity'].append(c)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Add section averages\n",
    "        features.extend([\n",
    "            np.mean(section_metrics['sample_entropy']) \n",
    "                if section_metrics['sample_entropy'] else np.nan,\n",
    "            np.mean(section_metrics['higuchi']) \n",
    "                if section_metrics['higuchi'] else np.nan,\n",
    "            np.mean(section_metrics['dfa']) \n",
    "                if section_metrics['dfa'] else np.nan,\n",
    "            np.mean(section_metrics['spectral_entropy']) \n",
    "                if section_metrics['spectral_entropy'] else np.nan,\n",
    "            np.mean(section_metrics['permutation_entropy']) \n",
    "                if section_metrics['permutation_entropy'] else np.nan,\n",
    "            np.mean(section_metrics['mobility']) \n",
    "                if section_metrics['mobility'] else np.nan,\n",
    "            np.mean(section_metrics['complexity']) \n",
    "                if section_metrics['complexity'] else np.nan\n",
    "        ])\n",
    "        \n",
    "        labels.extend([\n",
    "            f\"{section}_sample_entropy\",\n",
    "            f\"{section}_higuchi\",\n",
    "            f\"{section}_DFA\",\n",
    "            f\"{section}_spectral_entropy\",\n",
    "            f\"{section}_permutation_entropy\",\n",
    "            f\"{section}_hjorth_mobility\",\n",
    "            f\"{section}_hjorth_complexity\"\n",
    "        ])\n",
    "    \n",
    "    # Replace NaN with 0\n",
    "    features = [0 if np.isnan(x) else x for x in features]\n",
    "    \n",
    "    final = pd.DataFrame([features], columns=labels)\n",
    "    \n",
    "    if explicit:\n",
    "        print(f\"Other metrics shape: {final.shape}\")\n",
    "        print(final)\n",
    "    \n",
    "    return final\n",
    "    \n",
    "def overall_features(raw_frame, explicit=False): # This function combines the prior three feature extaction functions and returns a complete feature set for each data sample\n",
    "    df1 = band_powers_ratios(raw_frame, explicit=explicit)\n",
    "    df2 = db_wavelet_features(raw_frame, explicit=explicit)\n",
    "    df3 = other_metrics(raw_frame, explicit=explicit)\n",
    "\n",
    "    merged = pd.concat([df1, df2, df3], axis=1)\n",
    "    \n",
    "    if explicit:\n",
    "        print(merged.shape)\n",
    "        print()\n",
    "        print(list(merged.columns))\n",
    "        \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ec861fca-694d-44e3-b5ad-08dc4b220283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def processing():\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Create a list of the paths to each subject folder\n",
    "    sub_folders = []\n",
    "    for sub_id in os.listdir(\"Alzheimer-Frontotemporal-healthy/derivatives/\"):\n",
    "        if \"sub-\" in sub_id:\n",
    "            sub_folders.append(os.path.join(os.getcwd(), \"Alzheimer-Frontotemporal-healthy/derivatives/\", sub_id))\n",
    "    \n",
    "    # For each subject folder \n",
    "    for subject in (sorted(sub_folders)):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        subject_id = os.path.basename(subject)\n",
    "        edf_path = os.path.join(subject, \"eeg\", f\"{subject_id}_task-eyesclosed_eeg.set\")\n",
    "        print(edf_path)\n",
    "        \n",
    "        # If there is a missing file, skip this session\n",
    "        if not os.path.exists(edf_path):\n",
    "            print(f\"MISSING FILE ----------------------: {subject_id}\")\n",
    "            continue           \n",
    "\n",
    "        raw = mne.io.read_raw_eeglab(edf_path, preload=True)\n",
    "        raw_frame = raw.get_data(picks=\"eeg\") # Shape: (19, n_samples)\n",
    "        pd_frame = pd.DataFrame(raw_frame)\n",
    "        \n",
    "        skipped_chunks = 0\n",
    "\n",
    "            \n",
    "        # Create 4s segments of data\n",
    "        all_data_one_subject = []\n",
    "        n_samples = pd_frame.shape[1]\n",
    "        for start in range(0, n_samples, 1000):\n",
    "            end = start + 1000\n",
    "            chunk = pd_frame.iloc[:, start:end].astype(np.float32)\n",
    "            if chunk.shape[1] == 1000:\n",
    "                all_data_one_subject.append(overall_features(chunk))\n",
    "            else:\n",
    "                skipped_chunks += 1\n",
    "\n",
    "        if len(all_data_one_subject) == 0:\n",
    "            print(f\"No valid chunks found for {subject_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Concatenate all chunk-level features (chunks, features)\n",
    "        subject_features = pd.concat(all_data_one_subject, axis=0)\n",
    "        # Compute mean feature values across all chunks for this subject\n",
    "        final_one_subject = subject_features.mean(axis=0).to_frame().T  # (1, num_features)\n",
    "        # Add to list\n",
    "        all_data.append(final_one_subject)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Finished processing: {subject_id}\")\n",
    "        print(f\"Skipped {skipped_chunks} segments that were shorter than 4s at the end of the recordings.\")\n",
    "        print(f\"{subject_id} processed in {end_time-start_time} seconds\\n\\n\\n\\n\")\n",
    "        print(\"\\n\\n\\n\\n-----------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n\\n\\n\")\n",
    "        \n",
    "    return pd.concat(all_data, axis=0)\n",
    "\n",
    "# mne.set_log_level('WARNING')  # Or 'ERROR' to suppress even more output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e74457-e148-454a-814f-efc35be10b93",
   "metadata": {},
   "source": [
    "Compute features from preprocessed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "38d3920f-6598-4f84-96cc-9fbb5f68af31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complete_set = processing()\n",
    "complete_set.to_csv(\"all_features.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a6635195-20ef-49d5-827c-98e22b632b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Theta/Alpha</th>\n",
       "      <th>Theta/Beta</th>\n",
       "      <th>Delta/Alpha</th>\n",
       "      <th>cA4_RMS</th>\n",
       "      <th>cD4_RMS</th>\n",
       "      <th>cD3_RMS</th>\n",
       "      <th>...</th>\n",
       "      <th>parietal_permutation_entropy</th>\n",
       "      <th>parietal_hjorth_mobility</th>\n",
       "      <th>parietal_hjorth_complexity</th>\n",
       "      <th>occipital_sample_entropy</th>\n",
       "      <th>occipital_higuchi</th>\n",
       "      <th>occipital_DFA</th>\n",
       "      <th>occipital_spectral_entropy</th>\n",
       "      <th>occipital_permutation_entropy</th>\n",
       "      <th>occipital_hjorth_mobility</th>\n",
       "      <th>occipital_hjorth_complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>8.800000e+01</td>\n",
       "      <td>8.800000e+01</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-103.129312</td>\n",
       "      <td>-111.162507</td>\n",
       "      <td>-115.619760</td>\n",
       "      <td>-118.371847</td>\n",
       "      <td>3.146488</td>\n",
       "      <td>6.275635</td>\n",
       "      <td>20.964946</td>\n",
       "      <td>7.774880e-08</td>\n",
       "      <td>9.773483e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>1.549022</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>6.441718</td>\n",
       "      <td>0.202230</td>\n",
       "      <td>1.138733</td>\n",
       "      <td>1.645152</td>\n",
       "      <td>2.737385</td>\n",
       "      <td>1.554504</td>\n",
       "      <td>0.060304</td>\n",
       "      <td>5.762769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.649095</td>\n",
       "      <td>1.761380</td>\n",
       "      <td>1.114195</td>\n",
       "      <td>0.953315</td>\n",
       "      <td>1.212179</td>\n",
       "      <td>3.454211</td>\n",
       "      <td>4.375697</td>\n",
       "      <td>5.150870e-08</td>\n",
       "      <td>3.579450e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033278</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.824013</td>\n",
       "      <td>0.042561</td>\n",
       "      <td>0.037948</td>\n",
       "      <td>0.056845</td>\n",
       "      <td>0.194750</td>\n",
       "      <td>0.051057</td>\n",
       "      <td>0.011220</td>\n",
       "      <td>1.034896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-104.365970</td>\n",
       "      <td>-113.513469</td>\n",
       "      <td>-117.183120</td>\n",
       "      <td>-119.260657</td>\n",
       "      <td>1.153251</td>\n",
       "      <td>2.406482</td>\n",
       "      <td>5.351467</td>\n",
       "      <td>4.136917e-08</td>\n",
       "      <td>6.378793e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>1.476790</td>\n",
       "      <td>0.042350</td>\n",
       "      <td>4.212693</td>\n",
       "      <td>0.135519</td>\n",
       "      <td>1.071769</td>\n",
       "      <td>1.416028</td>\n",
       "      <td>2.400990</td>\n",
       "      <td>1.430122</td>\n",
       "      <td>0.044170</td>\n",
       "      <td>3.207159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-103.467207</td>\n",
       "      <td>-112.602107</td>\n",
       "      <td>-116.434366</td>\n",
       "      <td>-118.898763</td>\n",
       "      <td>2.336695</td>\n",
       "      <td>4.106344</td>\n",
       "      <td>18.161454</td>\n",
       "      <td>5.510485e-08</td>\n",
       "      <td>7.730037e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>1.531042</td>\n",
       "      <td>0.046973</td>\n",
       "      <td>5.951293</td>\n",
       "      <td>0.170763</td>\n",
       "      <td>1.115019</td>\n",
       "      <td>1.626078</td>\n",
       "      <td>2.577708</td>\n",
       "      <td>1.523003</td>\n",
       "      <td>0.051954</td>\n",
       "      <td>4.961396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-103.255175</td>\n",
       "      <td>-111.590079</td>\n",
       "      <td>-115.749031</td>\n",
       "      <td>-118.663800</td>\n",
       "      <td>2.772265</td>\n",
       "      <td>5.057567</td>\n",
       "      <td>20.915231</td>\n",
       "      <td>6.522598e-08</td>\n",
       "      <td>8.901056e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>1.545894</td>\n",
       "      <td>0.050990</td>\n",
       "      <td>6.574165</td>\n",
       "      <td>0.198331</td>\n",
       "      <td>1.133567</td>\n",
       "      <td>1.652868</td>\n",
       "      <td>2.703931</td>\n",
       "      <td>1.555053</td>\n",
       "      <td>0.058367</td>\n",
       "      <td>5.838881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-102.943712</td>\n",
       "      <td>-110.022473</td>\n",
       "      <td>-115.093136</td>\n",
       "      <td>-118.148545</td>\n",
       "      <td>3.599146</td>\n",
       "      <td>7.356218</td>\n",
       "      <td>24.156296</td>\n",
       "      <td>8.230835e-08</td>\n",
       "      <td>1.062063e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>1.571056</td>\n",
       "      <td>0.056887</td>\n",
       "      <td>7.089277</td>\n",
       "      <td>0.223833</td>\n",
       "      <td>1.153332</td>\n",
       "      <td>1.686990</td>\n",
       "      <td>2.878192</td>\n",
       "      <td>1.585870</td>\n",
       "      <td>0.066497</td>\n",
       "      <td>6.547607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-100.169830</td>\n",
       "      <td>-105.252137</td>\n",
       "      <td>-110.321474</td>\n",
       "      <td>-113.485023</td>\n",
       "      <td>7.611849</td>\n",
       "      <td>24.455624</td>\n",
       "      <td>30.528468</td>\n",
       "      <td>4.631469e-07</td>\n",
       "      <td>2.970969e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.629001</td>\n",
       "      <td>0.080672</td>\n",
       "      <td>7.946976</td>\n",
       "      <td>0.349816</td>\n",
       "      <td>1.259677</td>\n",
       "      <td>1.726333</td>\n",
       "      <td>3.298351</td>\n",
       "      <td>1.680960</td>\n",
       "      <td>0.098988</td>\n",
       "      <td>7.522396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Delta       Theta       Alpha        Beta  Theta/Alpha  \\\n",
       "count   88.000000   88.000000   88.000000   88.000000    88.000000   \n",
       "mean  -103.129312 -111.162507 -115.619760 -118.371847     3.146488   \n",
       "std      0.649095    1.761380    1.114195    0.953315     1.212179   \n",
       "min   -104.365970 -113.513469 -117.183120 -119.260657     1.153251   \n",
       "25%   -103.467207 -112.602107 -116.434366 -118.898763     2.336695   \n",
       "50%   -103.255175 -111.590079 -115.749031 -118.663800     2.772265   \n",
       "75%   -102.943712 -110.022473 -115.093136 -118.148545     3.599146   \n",
       "max   -100.169830 -105.252137 -110.321474 -113.485023     7.611849   \n",
       "\n",
       "       Theta/Beta  Delta/Alpha       cA4_RMS       cD4_RMS    cD3_RMS  ...  \\\n",
       "count   88.000000    88.000000  8.800000e+01  8.800000e+01  88.000000  ...   \n",
       "mean     6.275635    20.964946  7.774880e-08  9.773483e-07   0.000006  ...   \n",
       "std      3.454211     4.375697  5.150870e-08  3.579450e-07   0.000002  ...   \n",
       "min      2.406482     5.351467  4.136917e-08  6.378793e-07   0.000004  ...   \n",
       "25%      4.106344    18.161454  5.510485e-08  7.730037e-07   0.000005  ...   \n",
       "50%      5.057567    20.915231  6.522598e-08  8.901056e-07   0.000006  ...   \n",
       "75%      7.356218    24.156296  8.230835e-08  1.062063e-06   0.000007  ...   \n",
       "max     24.455624    30.528468  4.631469e-07  2.970969e-06   0.000017  ...   \n",
       "\n",
       "       parietal_permutation_entropy  parietal_hjorth_mobility  \\\n",
       "count                     88.000000                 88.000000   \n",
       "mean                       1.549022                  0.052425   \n",
       "std                        0.033278                  0.007514   \n",
       "min                        1.476790                  0.042350   \n",
       "25%                        1.531042                  0.046973   \n",
       "50%                        1.545894                  0.050990   \n",
       "75%                        1.571056                  0.056887   \n",
       "max                        1.629001                  0.080672   \n",
       "\n",
       "       parietal_hjorth_complexity  occipital_sample_entropy  \\\n",
       "count                   88.000000                 88.000000   \n",
       "mean                     6.441718                  0.202230   \n",
       "std                      0.824013                  0.042561   \n",
       "min                      4.212693                  0.135519   \n",
       "25%                      5.951293                  0.170763   \n",
       "50%                      6.574165                  0.198331   \n",
       "75%                      7.089277                  0.223833   \n",
       "max                      7.946976                  0.349816   \n",
       "\n",
       "       occipital_higuchi  occipital_DFA  occipital_spectral_entropy  \\\n",
       "count          88.000000      88.000000                   88.000000   \n",
       "mean            1.138733       1.645152                    2.737385   \n",
       "std             0.037948       0.056845                    0.194750   \n",
       "min             1.071769       1.416028                    2.400990   \n",
       "25%             1.115019       1.626078                    2.577708   \n",
       "50%             1.133567       1.652868                    2.703931   \n",
       "75%             1.153332       1.686990                    2.878192   \n",
       "max             1.259677       1.726333                    3.298351   \n",
       "\n",
       "       occipital_permutation_entropy  occipital_hjorth_mobility  \\\n",
       "count                      88.000000                  88.000000   \n",
       "mean                        1.554504                   0.060304   \n",
       "std                         0.051057                   0.011220   \n",
       "min                         1.430122                   0.044170   \n",
       "25%                         1.523003                   0.051954   \n",
       "50%                         1.555053                   0.058367   \n",
       "75%                         1.585870                   0.066497   \n",
       "max                         1.680960                   0.098988   \n",
       "\n",
       "       occipital_hjorth_complexity  \n",
       "count                    88.000000  \n",
       "mean                      5.762769  \n",
       "std                       1.034896  \n",
       "min                       3.207159  \n",
       "25%                       4.961396  \n",
       "50%                       5.838881  \n",
       "75%                       6.547607  \n",
       "max                       7.522396  \n",
       "\n",
       "[8 rows x 51 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = pd.read_csv(\"all_features.csv\")\n",
    "all_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fc622f9b-83ce-4ecc-86e6-aa4280b6e060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta -> Value: -103.579271494289\n",
      "Theta -> Value: -110.6226264453298\n",
      "Alpha -> Value: -116.20858785997082\n",
      "Beta -> Value: -119.026781491099\n",
      "Theta/Alpha -> Value: 4.050791410081116\n",
      "Theta/Beta -> Value: 7.902627690769325\n",
      "Delta/Alpha -> Value: 20.56709016322568\n",
      "cA4_RMS -> Value: 4.39660219342386e-08\n",
      "cD4_RMS -> Value: 7.187799724306387e-07\n",
      "cD3_RMS -> Value: 4.7548196562274825e-06\n",
      "cD2_RMS -> Value: 1.201431132358266e-05\n",
      "wavelet_entropy -> Value: 0.1264133900403976\n",
      "global_spectral_entropy -> Value: 2.647349549385849\n",
      "global_permutation_entropy -> Value: 1.537454524203489\n",
      "global_hjorth_mobility -> Value: 0.0533972755074501\n",
      "global_hjorth_complexity -> Value: 5.984928131103516\n",
      "frontal_sample_entropy -> Value: 0.187453946090347\n",
      "frontal_higuchi -> Value: 1.1410155665397637\n",
      "frontal_DFA -> Value: 1.6544103327901136\n",
      "frontal_spectral_entropy -> Value: 2.658856214668956\n",
      "frontal_permutation_entropy -> Value: 1.558284648267642\n",
      "frontal_hjorth_mobility -> Value: 0.0564203262329101\n",
      "frontal_hjorth_complexity -> Value: 5.965734481811523\n",
      "central_sample_entropy -> Value: 0.149674428569766\n",
      "central_higuchi -> Value: 1.124714703569297\n",
      "central_DFA -> Value: 1.7039669547395202\n",
      "central_spectral_entropy -> Value: 2.5030242942460816\n",
      "central_permutation_entropy -> Value: 1.5417998580541254\n",
      "central_hjorth_mobility -> Value: 0.046355701982975\n",
      "central_hjorth_complexity -> Value: 6.854597568511963\n",
      "temporal_sample_entropy -> Value: 0.1854715000193674\n",
      "temporal_higuchi -> Value: 1.1237567634773673\n",
      "temporal_DFA -> Value: 1.6755073392775983\n",
      "temporal_spectral_entropy -> Value: 2.7124847852816365\n",
      "temporal_permutation_entropy -> Value: 1.5345504733916082\n",
      "temporal_hjorth_mobility -> Value: 0.0554455555975437\n",
      "temporal_hjorth_complexity -> Value: 5.695793151855469\n",
      "parietal_sample_entropy -> Value: 0.1661977116697991\n",
      "parietal_higuchi -> Value: 1.1099864310009409\n",
      "parietal_DFA -> Value: 1.701893355713037\n",
      "parietal_spectral_entropy -> Value: 2.62753130963367\n",
      "parietal_permutation_entropy -> Value: 1.515314567282212\n",
      "parietal_hjorth_mobility -> Value: 0.0502732433378696\n",
      "parietal_hjorth_complexity -> Value: 5.952694892883301\n",
      "occipital_sample_entropy -> Value: 0.1817312145295944\n",
      "occipital_higuchi -> Value: 1.1020617236352157\n",
      "occipital_DFA -> Value: 1.6990980430096048\n",
      "occipital_spectral_entropy -> Value: 2.723020991441324\n",
      "occipital_permutation_entropy -> Value: 1.4970491262086747\n",
      "occipital_hjorth_mobility -> Value: 0.0539684295654296\n",
      "occipital_hjorth_complexity -> Value: 5.374223232269287\n"
     ]
    }
   ],
   "source": [
    "my_row = all_features.iloc[1] \n",
    "\n",
    "my_row_dict = my_row.to_dict()\n",
    "\n",
    "for column_name, value in my_row_dict.items():\n",
    "    print(f\"{column_name} -> Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a3b51813-717b-47db-8fe2-91c33e998c51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delta                            6.490947e-01\n",
       "Theta                            1.761380e+00\n",
       "Alpha                            1.114195e+00\n",
       "Beta                             9.533148e-01\n",
       "Theta/Alpha                      1.212179e+00\n",
       "Theta/Beta                       3.454211e+00\n",
       "Delta/Alpha                      4.375697e+00\n",
       "cA4_RMS                          5.150870e-08\n",
       "cD4_RMS                          3.579450e-07\n",
       "cD3_RMS                          2.014061e-06\n",
       "cD2_RMS                          3.014804e-06\n",
       "wavelet_entropy                  5.095408e-02\n",
       "global_spectral_entropy          1.491814e-01\n",
       "global_permutation_entropy       3.322473e-02\n",
       "global_hjorth_mobility           1.009485e-02\n",
       "global_hjorth_complexity         7.150698e-01\n",
       "frontal_sample_entropy           4.321709e-02\n",
       "frontal_higuchi                  3.336070e-02\n",
       "frontal_DFA                      6.558983e-02\n",
       "frontal_spectral_entropy         1.682291e-01\n",
       "frontal_permutation_entropy      3.557743e-02\n",
       "frontal_hjorth_mobility          1.333031e-02\n",
       "frontal_hjorth_complexity        8.045011e-01\n",
       "central_sample_entropy           2.223301e-02\n",
       "central_higuchi                  2.071151e-02\n",
       "central_DFA                      3.853508e-02\n",
       "central_spectral_entropy         1.188283e-01\n",
       "central_permutation_entropy      2.574069e-02\n",
       "central_hjorth_mobility          7.296237e-03\n",
       "central_hjorth_complexity        6.061643e-01\n",
       "temporal_sample_entropy          4.232155e-02\n",
       "temporal_higuchi                 3.402677e-02\n",
       "temporal_DFA                     6.394410e-02\n",
       "temporal_spectral_entropy        1.932123e-01\n",
       "temporal_permutation_entropy     4.084119e-02\n",
       "temporal_hjorth_mobility         1.457802e-02\n",
       "temporal_hjorth_complexity       8.044460e-01\n",
       "parietal_sample_entropy          2.559728e-02\n",
       "parietal_higuchi                 2.346431e-02\n",
       "parietal_DFA                     3.810038e-02\n",
       "parietal_spectral_entropy        1.514666e-01\n",
       "parietal_permutation_entropy     3.327838e-02\n",
       "parietal_hjorth_mobility         7.514337e-03\n",
       "parietal_hjorth_complexity       8.240132e-01\n",
       "occipital_sample_entropy         4.256141e-02\n",
       "occipital_higuchi                3.794778e-02\n",
       "occipital_DFA                    5.684508e-02\n",
       "occipital_spectral_entropy       1.947496e-01\n",
       "occipital_permutation_entropy    5.105692e-02\n",
       "occipital_hjorth_mobility        1.122042e-02\n",
       "occipital_hjorth_complexity      1.034896e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27bd39-be44-4770-bace-d13fad5b5519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check the coefficient of varation for the data\n",
    "CV > 2: Good variation, keep feature\n",
    "CV 0.1-0.2: Low variation, borderline useful\n",
    "CV < 0.1: Very low variation, likely not useful\n",
    "\"\"\"\n",
    "cv = all_features.std() / all_features.mean().abs()\n",
    "print(cv[['cA4_RMS', 'cD4_RMS', 'cD3_RMS', 'cD2_RMS', 'cD1_RMS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd98ddf-c2b2-4005-bd88-4938692581ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check gamma because of low variance, it has a constant value, need to drop this feature\n",
    "print(f\"Gamma min: {complete_set['Gamma'].min()}\")\n",
    "print(f\"Gamma max: {complete_set['Gamma'].max()}\")\n",
    "\n",
    "# Drop Gamma\n",
    "complete_set = complete_set.drop(columns=\"Gamma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140e863-a6dc-4f4f-8f55-f5d1091cd507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_features = all_features.drop(columns=\"cD1_RMS\")\n",
    "complete_set.to_csv(\"Alzheimer-Frontotemporal-healthy/all_features_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "eee8ee43-b9b9-4ac0-8581-855f42bacaba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_features_cleaned = pd.read_csv(\"Alzheimer-Frontotemporal-healthy/all_features_cleaned.csv\")\n",
    "sub_info = pd.read_csv(\"Alzheimer-Frontotemporal-healthy/participants.tsv\", sep=\"\\t\")\n",
    "final = pd.concat([sub_info, all_features_cleaned], axis=1)\n",
    "# final = final.drop(columns='Unnamed: 0') # Random column that appeared\n",
    "final.to_csv(\"Alzheimer-Frontotemporal-healthy/sub_info_and_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
