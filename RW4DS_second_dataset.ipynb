{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fd8f32-2203-4a96-ba89-34328e9ee8cd",
   "metadata": {},
   "source": [
    "# Alzheimer's disease, Frontotemporal dementia and Healthy subjects dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a2c366-f3ef-42d9-a6cf-58e5ac000fdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from mne.preprocessing import ICA\n",
    "import time\n",
    "\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import kurtosis, skew\n",
    "import pywt\n",
    "import antropy as ant"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "02d0fc52-aacb-4c5b-bc62-49e763a9331e",
   "metadata": {},
   "source": [
    "Firstly, two EEG files are imported to check their data and compare to see if there is anything off between files as well. Important to note is that these data files are already preprocessed by the original researchers themselves. \n",
    "\n",
    "Everything seems in normal ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36760859-cf3f-4dcd-b240-8cc04a08a521",
   "metadata": {
    "tags": []
   },
   "source": [
    "data = mne.io.read_raw_eeglab(\"Alzheimer-Frontotemporal-healthy/derivatives/sub-001/eeg/sub-001_task-eyesclosed_eeg.set\", preload=True)\n",
    "data.describe()\n",
    "data.plot()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "950a6fea-9e55-485c-b7d2-62eaad74012b",
   "metadata": {
    "tags": []
   },
   "source": [
    "raw = mne.io.read_raw_eeglab(\"Alzheimer-Frontotemporal-healthy/derivatives/sub-002/eeg/sub-002_task-eyesclosed_eeg.set\", preload=True)\n",
    "raw.describe()\n",
    "raw.plot()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a041f8ce-d638-4620-99ad-edbf4dc43e27",
   "metadata": {},
   "source": [
    "Here the feature extraction functions for the EEG data are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39d814b9-3132-4517-b05f-63c3ddf06277",
   "metadata": {
    "tags": []
   },
   "source": [
    "def band_powers_ratios(raw_frame, explicit=True): # raw frame is 128 x 75250\n",
    "    freqs, psd = welch(raw_frame, fs=250, nperseg=min(1000, raw_frame.shape[1]), axis=1)\n",
    "\n",
    "    # Define the frequency ranges of the five main brain wavelength bands\n",
    "    freq_bands = {\n",
    "        \"Delta\": [1, 4], \n",
    "        \"Theta\": [4, 8],\n",
    "        \"Alpha\": [8, 13],\n",
    "        \"Beta\":  [13, 30],\n",
    "        \"Gamma\": [30, 45]\n",
    "        }\n",
    "\n",
    "    # Calculate global average band power for entire sample\n",
    "    powers = {}\n",
    "    for band, (l, h) in freq_bands.items():\n",
    "        mask = (freqs >= l) & (freqs <= h)\n",
    "        # Take average across frequency bands, and then aggregate across channels to obtain a global metric\n",
    "        band_psd = np.mean(psd[:, mask], axis=1) # Shape (n_channels,)\n",
    "        powers[band] = np.mean(band_psd)  # Keep in µV²/Hz\n",
    "    \n",
    "    # Calculate ratios between global average band powers\n",
    "    ratios = {\n",
    "        \"Theta/Alpha\": powers[\"Theta\"] / powers[\"Alpha\"], # Classic Dementia biomarker\n",
    "        \"Theta/Beta\":  powers[\"Theta\"] / powers[\"Beta\"], # Correlates with cognitive decline\n",
    "        \"Delta/Alpha\": powers[\"Delta\"] / powers[\"Alpha\"] # Also considered a good indicator of cognitive decline\n",
    "    }\n",
    "    \n",
    "    # Convert the Power Spectral Density (PSD) to decibels, its standard measure\n",
    "    powers_db = {band: 10 * np.log10(val + 1e-12) for band, val in powers.items()}\n",
    "    \n",
    "    merged = {**powers_db, **ratios}\n",
    "    merged_dataframe = pd.DataFrame([merged])\n",
    "    \n",
    "    if explicit:\n",
    "        print(merged_dataframe.shape)\n",
    "        print(merged_dataframe)\n",
    "        \n",
    "    return merged_dataframe\n",
    "\n",
    "\n",
    "# Discrete Wavelet Transform - decompose a signal into different frequency components at different time scales\n",
    "# Unlike Fourier Transform, it can tell you at what time a frequency exists in the signal\n",
    "def db_wavelet_features(raw_frame, wavelet=\"db4\", level=4, explicit=True):\n",
    "    # Store features for each channel, as it works on 1D data\n",
    "    all_channel_features = []\n",
    "    \n",
    "    for ch_idx in range(raw_frame.shape[0]):\n",
    "        channel_features = []\n",
    "        channel_data = raw_frame.iloc[ch_idx, :].values.astype(np.float32)\n",
    "        \n",
    "        # Perform Discrete Wavelet Transform using Daubechies\n",
    "        \n",
    "        # Daubechies wavelets are a specific family of wavelet functions with good properties for EEG data\n",
    "        # We use \"wavelet=db4\" because the 4th order offers good balance between smoothness, localization and computational efficiency\n",
    "        coefs = pywt.wavedec(channel_data, wavelet=wavelet, level=level)\n",
    "        # We list them in reversed order to gain the standard ordering:\n",
    "        # cA4 - slow, overall trend; cD4:cD1 - go from slower to faster (increase in Herz range they investigate)\n",
    "        for i, c in reversed(list(enumerate(coefs))):\n",
    "            # We take the Root Mean Square as it is directly related to signal power. \n",
    "            # It measures how much power/energy is in a specific frequency component and has desirable mathematical properties for this use case\n",
    "            channel_features.extend([np.sqrt(np.mean(np.square(c)))])\n",
    "\n",
    "        # Extracting global wavelet entropy - it captures how evenly energy is distributed across frequency bands, in time-frequency domain using wavelet decomposition\n",
    "        # Captures major band imbalances and preserves temporal information about when those imbalances occur.\n",
    "        \n",
    "        # Calcuate energy per wavelet\n",
    "        energies = np.array([np.sum(np.square(c)) for c in coefs])\n",
    "        # See how much they contribute to the overall signal\n",
    "        distribution = energies / (np.sum(energies)+1e-12)\n",
    "        # Calculate Shannon entropy on the distribution\n",
    "        entropy = -np.sum(distribution * (np.log2(distribution)+1e-12))\n",
    "        channel_features.append(entropy) \n",
    "        \n",
    "        all_channel_features.append(channel_features)\n",
    "\n",
    "    # Average features across all channels\n",
    "    avg_features = np.mean(all_channel_features, axis=0)\n",
    "    \n",
    "    # Generate feature labels\n",
    "    labels = []\n",
    "    labels.extend([\"cA4_RMS\"])\n",
    "    labels.extend([f\"cD{i}_{'RMS'}\" for i in range(4,0,-1)])\n",
    "    labels.append(\"wavelet_entropy\")\n",
    "\n",
    "    # Combine the data with the labels to create a dataframe\n",
    "    final = pd.DataFrame(data=[avg_features], columns=labels)\n",
    "\n",
    "    if explicit:\n",
    "        print(final.shape)\n",
    "        print(final)\n",
    "    \n",
    "    return final\n",
    "\n",
    "def other_metrics(raw_frame, explicit=True):\n",
    "    # Asign all of the electrodes to five main head areas\n",
    "    head_sections = {\n",
    "        'frontal': [0, 1, 2, 3, 10, 11, 16],      # Fp1, Fp2, F3, F4, F7, F8, Fz\n",
    "        'central': [4, 5, 17],                     # C3, C4, Cz\n",
    "        'temporal': [12, 13, 14, 15],              # T3, T4, T5, T6\n",
    "        'parietal': [6, 7, 18],                    # P3, P4, Pz\n",
    "        'occipital': [8, 9]                        # O1, O2\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Global metrics (across all electrodes) - averaged across all channels\n",
    "    spectral_entropies = [] # measures how evenly power is distributed across all frequencies in the frequency domain, using Fourier Transform. \n",
    "                            # more sensitive to subtle frequency shifts\n",
    "    perm_entropies = [] # measures complexity by looking at the order/patterns of consecutive data points\n",
    "    mobility_values = [] # measures the \"mean frequency\" of how fast the signal oscillates\n",
    "    complexity_values = [] # measures how much the signal deviates from a simple sine wave\n",
    "    \n",
    "    for ch_idx in range(raw_frame.shape[0]):\n",
    "        channel_data = raw_frame.iloc[ch_idx,:].values.astype(np.float32)\n",
    "        \n",
    "        try:\n",
    "            spectral_entropies.append(ant.spectral_entropy(channel_data, sf=250))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            perm_entropies.append(ant.perm_entropy(channel_data))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            m, c = ant.hjorth_params(channel_data)\n",
    "            mobility_values.append(m)\n",
    "            complexity_values.append(c)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Add global averages\n",
    "    features.extend([\n",
    "        np.mean(spectral_entropies) if spectral_entropies else np.nan,\n",
    "        np.mean(perm_entropies) if perm_entropies else np.nan,\n",
    "        np.mean(mobility_values) if mobility_values else np.nan,\n",
    "        np.mean(complexity_values) if complexity_values else np.nan\n",
    "    ])\n",
    "    \n",
    "    labels.extend([\n",
    "        \"global_spectral_entropy\",\n",
    "        \"global_permutation_entropy\",\n",
    "        \"global_hjorth_mobility\",\n",
    "        \"global_hjorth_complexity\"\n",
    "    ])\n",
    "    \n",
    "    # Compute features for each head section, not global\n",
    "    \n",
    "    # Process each head section\n",
    "    for section, indices in head_sections.items():\n",
    "        section_metrics = {\n",
    "            # The reason the first three features are not computed on a gloval scale is that they are sensitive mainly to localized \n",
    "            # behavior in EEG signals, and averaging them across the entire scalp would obscure critical regional variations.\n",
    "            'sample_entropy': [], # measures complexity by checking how often similar patterns repeat in the signal\n",
    "            'higuchi': [], # Higuchi Fractal Dimension - measures the \"self-similarity\" or \"roughness\" of the signal across different time scales - how jagged or smooth the waveform is\n",
    "            'dfa': [], # Detrended Fluctuation Analysis - measures long-range correlations in the signal\n",
    "            'spectral_entropy': [],\n",
    "            'permutation_entropy': [],\n",
    "            'mobility': [],\n",
    "            'complexity': []\n",
    "        }\n",
    "        \n",
    "        for idx in indices:\n",
    "            electrode_data = raw_frame.iloc[idx, :].values.astype(np.float32)\n",
    "            electrode_data = np.ascontiguousarray(electrode_data)\n",
    "            \n",
    "            try:\n",
    "                section_metrics['sample_entropy'].append(ant.sample_entropy(electrode_data))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                section_metrics['higuchi'].append(ant.higuchi_fd(electrode_data))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                section_metrics['dfa'].append(ant.detrended_fluctuation(electrode_data))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                section_metrics['spectral_entropy'].append(\n",
    "                    ant.spectral_entropy(electrode_data, sf=250))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                section_metrics['permutation_entropy'].append(\n",
    "                    ant.perm_entropy(electrode_data))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                m, c = ant.hjorth_params(electrode_data)\n",
    "                section_metrics['mobility'].append(m)\n",
    "                section_metrics['complexity'].append(c)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Add section averages\n",
    "        features.extend([\n",
    "            np.mean(section_metrics['sample_entropy']) \n",
    "                if section_metrics['sample_entropy'] else np.nan,\n",
    "            np.mean(section_metrics['higuchi']) \n",
    "                if section_metrics['higuchi'] else np.nan,\n",
    "            np.mean(section_metrics['dfa']) \n",
    "                if section_metrics['dfa'] else np.nan,\n",
    "            np.mean(section_metrics['spectral_entropy']) \n",
    "                if section_metrics['spectral_entropy'] else np.nan,\n",
    "            np.mean(section_metrics['permutation_entropy']) \n",
    "                if section_metrics['permutation_entropy'] else np.nan,\n",
    "            np.mean(section_metrics['mobility']) \n",
    "                if section_metrics['mobility'] else np.nan,\n",
    "            np.mean(section_metrics['complexity']) \n",
    "                if section_metrics['complexity'] else np.nan\n",
    "        ])\n",
    "        \n",
    "        # Add feature labels\n",
    "        labels.extend([\n",
    "            f\"{section}_sample_entropy\",\n",
    "            f\"{section}_higuchi\",\n",
    "            f\"{section}_DFA\",\n",
    "            f\"{section}_spectral_entropy\",\n",
    "            f\"{section}_permutation_entropy\",\n",
    "            f\"{section}_hjorth_mobility\",\n",
    "            f\"{section}_hjorth_complexity\"\n",
    "        ])\n",
    "    \n",
    "    # Replace NaN with 0\n",
    "    features = [0 if np.isnan(x) else x for x in features]\n",
    "    \n",
    "    final = pd.DataFrame([features], columns=labels)\n",
    "    \n",
    "    if explicit:\n",
    "        print(f\"Other metrics shape: {final.shape}\")\n",
    "        print(final)\n",
    "    \n",
    "    return final\n",
    "    \n",
    "# This function combines the prior three feature extaction functions and returns a complete feature set for each data sample\n",
    "def overall_features(raw_frame, explicit=False): \n",
    "    df1 = band_powers_ratios(raw_frame, explicit=explicit)\n",
    "    df2 = db_wavelet_features(raw_frame, explicit=explicit)\n",
    "    df3 = other_metrics(raw_frame, explicit=explicit)\n",
    "\n",
    "    merged = pd.concat([df1, df2, df3], axis=1)\n",
    "    \n",
    "    if explicit:\n",
    "        print(merged.shape)\n",
    "        print()\n",
    "        print(list(merged.columns))\n",
    "        \n",
    "    return merged"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6dcf716e-059d-4d47-a22d-6db66c441d3d",
   "metadata": {},
   "source": [
    "Nextly, a function is defined that iterates through all the EEG files browsing through the subdirectories, loads the files and converts them to pd.DataFrame. After that, each subject's EEG file is split into 4s chunks and the features are computed on each chunk. Thereafter the average across all chunks is taken for one subject which forms the final features for that subject. This is done because EEG is very dynamic and calculating features on the entirety of these long recordings would not capture anything relevant. \n",
    "The final result is a concatenation of the features calculated for each subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec861fca-694d-44e3-b5ad-08dc4b220283",
   "metadata": {
    "tags": []
   },
   "source": [
    "def processing():\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Create a list of the paths to each subject folder\n",
    "    sub_folders = []\n",
    "    for sub_id in os.listdir(\"Alzheimer-Frontotemporal-healthy/derivatives/\"):\n",
    "        if \"sub-\" in sub_id:\n",
    "            sub_folders.append(os.path.join(os.getcwd(), \"Alzheimer-Frontotemporal-healthy/derivatives/\", sub_id))\n",
    "    \n",
    "    # For each subject folder \n",
    "    for subject in (sorted(sub_folders)):\n",
    "        # Take the time to understand how fast processing one subject is\n",
    "        start_time = time.time()\n",
    "        \n",
    "        subject_id = os.path.basename(subject)\n",
    "        edf_path = os.path.join(subject, \"eeg\", f\"{subject_id}_task-eyesclosed_eeg.set\")\n",
    "        print(edf_path)\n",
    "        \n",
    "        # If there is a missing file, skip this session\n",
    "        if not os.path.exists(edf_path):\n",
    "            print(f\"MISSING FILE ----------------------: {subject_id}\")\n",
    "            continue           \n",
    "\n",
    "        # Load the EEG file and convert to a Pandas dataframe\n",
    "        raw = mne.io.read_raw_eeglab(edf_path, preload=True)\n",
    "        raw_frame = raw.get_data(picks=\"eeg\") # Shape: (19, n_samples)\n",
    "        pd_frame = pd.DataFrame(raw_frame)\n",
    "        \n",
    "        # Calculate how many chunks of less than 4s data are cut out, because they were at the end of the recording\n",
    "        skipped_chunks = 0\n",
    "            \n",
    "        # Create 4s segments of data. Recording frequency is 250 Hz, so 1000 samples are 4s.\n",
    "        all_data_one_subject = []\n",
    "        n_samples = pd_frame.shape[1]\n",
    "        for start in range(0, n_samples, 1000):\n",
    "            end = start + 1000\n",
    "            chunk = pd_frame.iloc[:, start:end].astype(np.float32)\n",
    "            if chunk.shape[1] == 1000:\n",
    "                all_data_one_subject.append(overall_features(chunk))\n",
    "            else:\n",
    "                skipped_chunks += 1\n",
    "\n",
    "        # Some minor issue catching\n",
    "        if len(all_data_one_subject) == 0:\n",
    "            print(f\"No valid chunks found for {subject_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Concatenate all chunk-level features (chunks, features)\n",
    "        subject_features = pd.concat(all_data_one_subject, axis=0)\n",
    "        \n",
    "        # For Power Spectral Density (PSD) features, we need to take the mean in linear space, and then reconvert to dB\n",
    "        power_cols = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\"]\n",
    "        for col in power_cols:\n",
    "            # convert to linear from decibel\n",
    "            linear_powers = 10 ** (subject_features[col]/10)\n",
    "            # take the mean in linear space\n",
    "            mean_linear = linear_powers.mean()\n",
    "            # convert back to decibels\n",
    "            subject_features[col] = 10 * np.log10(mean_linear + 1e-12) # add negligible modifier to avoid log(0)\n",
    "            \n",
    "        \n",
    "        # Compute mean feature values across all chunks for this subject / for all else besides PSD no conversion needed\n",
    "        final_one_subject = subject_features.mean(axis=0).to_frame().T  # (1, num_features)\n",
    "        # Add to list of data for all subjects\n",
    "        all_data.append(final_one_subject)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Finished processing: {subject_id}\")\n",
    "        print(f\"Skipped {skipped_chunks} segments that were shorter than 4s at the end of the recordings.\")\n",
    "        print(f\"{subject_id} processed in {end_time-start_time} seconds\\n\\n\\n\\n\")\n",
    "        print(\"\\n\\n\\n\\n-----------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n\\n\\n\")\n",
    "        \n",
    "    return pd.concat(all_data, axis=0)\n",
    "\n",
    "# If suppression of output is desired, uncomment the below line. \n",
    "# mne.set_log_level('WARNING')  # Or 'ERROR' to suppress even more output "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91e74457-e148-454a-814f-efc35be10b93",
   "metadata": {},
   "source": [
    "Compute features from preprocessed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38d3920f-6598-4f84-96cc-9fbb5f68af31",
   "metadata": {
    "tags": []
   },
   "source": [
    "complete_set = processing()\n",
    "complete_set.to_csv(\"all_features_new.csv\", header=True, index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7f26abb8-85b0-494d-8359-f7a6de8a2cc4",
   "metadata": {},
   "source": [
    "Perform some analysis of the newly-computed features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6635195-20ef-49d5-827c-98e22b632b98",
   "metadata": {
    "tags": []
   },
   "source": [
    "all_features = pd.read_csv(\"all_features.csv\")\n",
    "all_features.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc622f9b-83ce-4ecc-86e6-aa4280b6e060",
   "metadata": {
    "tags": []
   },
   "source": [
    "my_row = all_features.iloc[1] \n",
    "\n",
    "my_row_dict = my_row.to_dict()\n",
    "\n",
    "for column_name, value in my_row_dict.items():\n",
    "    print(f\"{column_name} -> Value: {value}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3b51813-717b-47db-8fe2-91c33e998c51",
   "metadata": {
    "tags": []
   },
   "source": [
    "all_features.std()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f27bd39-be44-4770-bace-d13fad5b5519",
   "metadata": {
    "tags": []
   },
   "source": [
    "\"\"\"\n",
    "Check the coefficient of varation for the data\n",
    "CV > 2: Good variation, keep feature\n",
    "CV 0.1-0.2: Low variation, borderline useful\n",
    "CV < 0.1: Very low variation, likely not useful\n",
    "\"\"\"\n",
    "cv = all_features.std() / all_features.mean().abs()\n",
    "print(cv[['cA4_RMS', 'cD4_RMS', 'cD3_RMS', 'cD2_RMS', 'cD1_RMS']])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bcb5585-45c7-4be5-be7b-d73c0558ec65",
   "metadata": {
    "tags": []
   },
   "source": [
    "all_features = all_features.drop(columns=\"cD1_RMS\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dd98ddf-c2b2-4005-bd88-4938692581ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Check gamma because of low variance, it has a constant value, need to drop this feature\n",
    "print(f\"Gamma min: {complete_set['Gamma'].min()}\")\n",
    "print(f\"Gamma max: {complete_set['Gamma'].max()}\")\n",
    "\n",
    "# Drop Gamma\n",
    "complete_set = complete_set.drop(columns=\"Gamma\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2140e863-a6dc-4f4f-8f55-f5d1091cd507",
   "metadata": {
    "tags": []
   },
   "source": [
    "complete_set.to_csv(\"Alzheimer-Frontotemporal-healthy/all_features_cleaned.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8fad2178-9d40-4c70-a876-e3eb08b946f5",
   "metadata": {},
   "source": [
    "# Finally, combine the individual subject information gathered in the study with the features computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eee8ee43-b9b9-4ac0-8581-855f42bacaba",
   "metadata": {
    "tags": []
   },
   "source": [
    "all_features_cleaned = pd.read_csv(\"Alzheimer-Frontotemporal-healthy/all_features_cleaned.csv\")\n",
    "sub_info = pd.read_csv(\"Alzheimer-Frontotemporal-healthy/participants.tsv\", sep=\"\\t\")\n",
    "final = pd.concat([sub_info, all_features_cleaned], axis=1)\n",
    "final = final.drop(columns='Unnamed: 0') # Random column that appeared\n",
    "final.to_csv(\"Alzheimer-Frontotemporal-healthy/sub_info_and_features.csv\", index=False)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
